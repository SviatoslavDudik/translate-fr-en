{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4N9AD2dlrsU"
   },
   "source": [
    "# French to English machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CfvojjHQlrsU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 1.3.0 not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch>=1.3.0\n",
    "!pip3 install subword-nmt &> log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PX-ehjMA78K8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "--2021-12-12 17:42:00--  https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1837696 (1,8M) [text/plain]\n",
      "Saving to: ‘data/train.en’\n",
      "\n",
      "data/train.en       100%[===================>]   1,75M  8,70MB/s    in 0,2s    \n",
      "\n",
      "2021-12-12 17:42:00 (8,70 MB/s) - ‘data/train.en’ saved [1837696/1837696]\n",
      "\n",
      "--2021-12-12 17:42:00--  https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.fr\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2235588 (2,1M) [text/plain]\n",
      "Saving to: ‘data/train.fr’\n",
      "\n",
      "data/train.fr       100%[===================>]   2,13M  10,7MB/s    in 0,2s    \n",
      "\n",
      "2021-12-12 17:42:01 (10,7 MB/s) - ‘data/train.fr’ saved [2235588/2235588]\n",
      "\n",
      "--2021-12-12 17:42:01--  https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/val.lc.norm.tok.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64608 (63K) [text/plain]\n",
      "Saving to: ‘data/val.en’\n",
      "\n",
      "data/val.en         100%[===================>]  63,09K  --.-KB/s    in 0,007s  \n",
      "\n",
      "2021-12-12 17:42:01 (9,20 MB/s) - ‘data/val.en’ saved [64608/64608]\n",
      "\n",
      "--2021-12-12 17:42:01--  https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/val.lc.norm.tok.fr\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 78242 (76K) [text/plain]\n",
      "Saving to: ‘data/val.fr’\n",
      "\n",
      "data/val.fr         100%[===================>]  76,41K  --.-KB/s    in 0,006s  \n",
      "\n",
      "2021-12-12 17:42:01 (12,5 MB/s) - ‘data/val.fr’ saved [78242/78242]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!rm -f data/*\n",
    "!wget -O data/train.en https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.en \n",
    "!wget -O data/train.fr https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.fr\n",
    "!wget -O data/val.en https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/val.lc.norm.tok.en\n",
    "!wget -O data/val.fr https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/val.lc.norm.tok.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g9kP0SdxlrsY"
   },
   "outputs": [],
   "source": [
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "# build and apply bpe vocs\n",
    "bpe = {}\n",
    "for lang in ['en', 'fr']:\n",
    "    learn_bpe(open('data/train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
    "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
    "    \n",
    "    with open('train.bpe.' + lang, 'w') as f_out:\n",
    "        for line in open('data/train.' + lang):\n",
    "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UPW3sV8lrsb"
   },
   "source": [
    "### Building vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CmTy_m_olrsb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8PskgBSxlrsd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: un homme descend des marches en skateboard .\n",
      "out: a man is skating down a set of stairs .\n",
      "\n",
      "inp: deux hommes dans la soi@@ x@@ an@@ taine qui jouent aux dames dans un parc\n",
      "out: two man in their 6@@ 0s playing checkers at the park\n",
      "\n",
      "inp: deux chiens , un blanc et un marron , courent l&apos; un à côté de l&apos; autre près d&apos; une prairie .\n",
      "out: two dogs , one white and one brown , run along side each other near a grassy field .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_inp = np.array(open('./train.bpe.fr').read().split('\\n'))\n",
    "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
    "                                                          random_state=42)\n",
    "for i in range(3):\n",
    "    print('inp:', train_inp[i])\n",
    "    print('out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ogp-bjwtrmuI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens, bos=\"_BOS_\", eos=\"_EOS_\", unk='_UNK_'):\n",
    "        \"\"\"\n",
    "        A special class that converts lines of tokens into matrices and backwards\n",
    "        \"\"\"\n",
    "        assert all(tok in tokens for tok in (bos, eos, unk))\n",
    "        self.tokens = tokens\n",
    "        self.token_to_ix = {t:i for i, t in enumerate(tokens)}\n",
    "        self.bos, self.eos, self.unk = bos, eos, unk\n",
    "        self.bos_ix = self.token_to_ix[bos]\n",
    "        self.eos_ix = self.token_to_ix[eos]\n",
    "        self.unk_ix = self.token_to_ix[unk]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_lines(lines, bos=\"_BOS_\", eos=\"_EOS_\", unk='_UNK_'):\n",
    "        flat_lines = '\\n'.join(list(lines)).split()\n",
    "        tokens = sorted(set(flat_lines))\n",
    "        tokens = [t for t in tokens if t not in (bos, eos, unk) and len(t)]\n",
    "        tokens = [bos, eos, unk] + tokens\n",
    "        return Vocab(tokens, bos, eos, unk)\n",
    "\n",
    "    def tokenize(self, string):\n",
    "        \"\"\"converts string to a list of tokens\"\"\"\n",
    "        tokens = [tok if tok in self.token_to_ix else self.unk\n",
    "                  for tok in string.split()]\n",
    "        return [self.bos] + tokens + [self.eos]\n",
    "\n",
    "    def to_matrix(self, lines, dtype=torch.int64, max_len=None):\n",
    "        \"\"\"\n",
    "        convert variable length token sequences into fixed size matrix\n",
    "        example usage:\n",
    "        >>>print(to_matrix(words[:3],source_to_ix))\n",
    "        [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n",
    "         [30 21 15 15 21 14 28 27 13 -1 -1]\n",
    "         [25 37 31 34 21 20 37 21 28 19 13]]\n",
    "        \"\"\"\n",
    "        lines = list(map(self.tokenize, lines))\n",
    "        max_len = max_len or max(map(len, lines))\n",
    "\n",
    "        matrix = torch.full((len(lines), max_len), self.eos_ix, dtype=dtype)\n",
    "        for i, seq in enumerate(lines):\n",
    "            row_ix = list(map(self.token_to_ix.get, seq))[:max_len]\n",
    "            matrix[i, :len(row_ix)] = torch.as_tensor(row_ix)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def to_lines(self, matrix, crop=True):\n",
    "        \"\"\"\n",
    "        Convert matrix of token ids into strings\n",
    "        :param matrix: matrix of tokens of int32, shape=[batch,time]\n",
    "        :param crop: if True, crops BOS and EOS from line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        for line_ix in map(list,matrix):\n",
    "            if crop:\n",
    "                if line_ix[0] == self.bos_ix:\n",
    "                    line_ix = line_ix[1:]\n",
    "                if self.eos_ix in line_ix:\n",
    "                    line_ix = line_ix[:line_ix.index(self.eos_ix)]\n",
    "            line = ' '.join(self.tokens[i] for i in line_ix)\n",
    "            lines.append(line)\n",
    "        return lines\n",
    "    \n",
    "    def compute_mask(self, input_ix):\n",
    "        \"\"\" compute a boolean mask that equals \"1\" until first EOS (including that EOS) \"\"\"\n",
    "        return F.pad(torch.cumsum(input_ix == self.eos_ix, dim=-1)[..., :-1] < 1, pad=(1, 0, 0, 0), value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vipg4O61lrsg"
   },
   "outputs": [],
   "source": [
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cwOoHfuhlrsi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "['un petit garçon saute .', 'un chien s&apos; ébroue', 'un homme peint un mur .', 'deux hommes se parlant .', 'un paysage de montagne .']\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "tensor([[   0, 6622, 4500, 2678, 5566,   19,    1,    1],\n",
      "        [   0, 6622, 1276, 5495, 6996,    1,    1,    1],\n",
      "        [   0, 6622, 2939, 4433, 6622, 3816,   19,    1],\n",
      "        [   0, 1844, 2940, 5610, 4327,   19,    1,    1],\n",
      "        [   0, 6622, 4419, 1790, 3730,   19,    1,    1]])\n",
      "\n",
      "back to words\n",
      "['un petit garçon saute .', 'un chien s&apos; ébroue', 'un homme peint un mur .', 'deux hommes se parlant .', 'un paysage de montagne .']\n"
     ]
    }
   ],
   "source": [
    "batch_lines = sorted(train_inp, key=len)[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSYu-MkElrsk"
   },
   "source": [
    "Draw source and translation length distributions to estimate the scope of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TLLl9cSNlrsl"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHWgx34flrsn"
   },
   "source": [
    "### Encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pd_rDRm9lrso"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wgfN5-F7lrst"
   },
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
    "        \"\"\"\n",
    "        A simple encoder-decoder seq2seq model\n",
    "        \"\"\"\n",
    "        super().__init__() # initialize base class to track sub-layers, parameters, etc.\n",
    "\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "    def forward(self, inp, out):\n",
    "        \"\"\" Apply model in training mode \"\"\"\n",
    "        initial_state = self.encode(inp)\n",
    "        return self.decode(initial_state, out)\n",
    "\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :returns: initial decoder state tensors, one or many\n",
    "        \"\"\"\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)\n",
    "        return [dec_start]\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]\n",
    "        \"\"\"\n",
    "        prev_gru0_state = prev_state[0]\n",
    "        \n",
    "        prev_ix = self.emb_out(prev_tokens)\n",
    "        output = self.dec0(prev_ix, prev_gru0_state)\n",
    "        output_logits = self.logits(output)\n",
    "        new_dec_state = [output]\n",
    "        \n",
    "        return new_dec_state, output_logits\n",
    "\n",
    "    def decode(self, initial_state, out_tokens, **flags):\n",
    "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
    "        batch_size = out_tokens.shape[0]\n",
    "        state = initial_state\n",
    "        \n",
    "        # initial logits: always predict BOS\n",
    "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
    "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
    "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
    "        \n",
    "        logits_sequence = [first_logits]\n",
    "        for i in range(out_tokens.shape[1] - 1):\n",
    "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
    "            logits_sequence.append(logits)\n",
    "        return torch.stack(logits_sequence, dim=1)\n",
    "\n",
    "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
    "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
    "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
    "        state = initial_state\n",
    "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64, \n",
    "                              device=device)]\n",
    "        all_states = [initial_state]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            state, logits = self.decode_step(state, outputs[-1])\n",
    "            outputs.append(logits.argmax(dim=-1))\n",
    "            all_states.append(state)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1), all_states\n",
    "\n",
    "    def translate_lines(self, inp_lines, **kwargs):\n",
    "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
    "        initial_state = self.encode(inp)\n",
    "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
    "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8cuXPNhB7Yod"
   },
   "outputs": [],
   "source": [
    "# debugging area\n",
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "\n",
    "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
    "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
    "\n",
    "h0 = model.encode(dummy_inp_tokens)\n",
    "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
    "\n",
    "assert isinstance(h1, list) and len(h1) == len(h0)\n",
    "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
    "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
    "\n",
    "logits_seq = model.decode(h0, dummy_out_tokens)\n",
    "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
    "\n",
    "# full forward\n",
    "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
    "assert logits_seq2.shape == logits_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "isQ3deTU7Yoe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations without training:\n",
      "stun@@ shop motorcycle silly ballerinas adole@@ life descend arch@@ bly pa@@ swimsuits civil expanse trumpets bite unloading unloading centr@@ tric streetlight tie-@@ surge@@ name@@ wicker\n",
      "stun@@ bib ations pped ings ford headbands middle-@@ mowing meat daylight o-@@ clams cau@@ people descend arch@@ bly sin@@ butcher traditional spandex traditional ading kitten\n",
      "leaf placed centr@@ screaming grills mouths blanket elabor@@ traditional out arrow enthusiastically enjoys keeping on@@ section floral stuff@@ deserted trick sting floppy sparkler bea@@ bends\n"
     ]
    }
   ],
   "source": [
    "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)\n",
    "print(\"Translations without training:\")\n",
    "print('\\n'.join([line for line in dummy_translations]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wuv1-aVlrs0"
   },
   "source": [
    "### Training loss (2 points)\n",
    "\n",
    "Our training objective is almost the same as it was for neural language models:\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
    "\n",
    "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c8XPV8sWlrs5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    Compute loss (float32 scalar) as in the formula above\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    \n",
    "    \"\"\"\n",
    "    mask = model.out_voc.compute_mask(out) # [batch_size, out_len]\n",
    "    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)\n",
    "    \n",
    "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
    "    logits_seq = model(inp, out)\n",
    "\n",
    "    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]\n",
    "    logprobs_seq = F.log_softmax(logits_seq, dim=-1)\n",
    "   \n",
    "    # log-probabilities of correct outputs, [batch_size, out_len]\n",
    "    logp_out = (logprobs_seq * targets_1hot).sum(dim=-1)\n",
    "    # ^-- this will select the probability of the actual next token.\n",
    "    # Note: you can compute loss more efficiently using using F.cross_entropy\n",
    "\n",
    "    # average cross-entropy over tokens where mask == True\n",
    "    return  -1 * torch.mean(logp_out[mask], dtype=torch.float32) # average loss, scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iEMRd4ebluZU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([[1,2,3],[1,2,3]]), 5).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ME_LWUeklrs7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.6161, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
    "print(\"Loss:\", dummy_loss)\n",
    "\n",
    "# test autograd\n",
    "dummy_loss.backward()\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.grad is not None and abs(param.grad.max()) != 0, f\"Param {name} received no gradients\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpbaBpW7lrs-"
   },
   "source": [
    "### Evaluation: BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Gb1-PhKIlrs-"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\"\n",
    "    Estimates corpora-level BLEU score of model's translations given inp and reference out\n",
    "    Note: if you're serious about reporting your results, use https://pypi.org/project/sacrebleu\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
    "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
    "        return corpus_bleu(\n",
    "            [[ref.split()] for ref in actual],\n",
    "            [trans.split() for trans in translations],\n",
    "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
    "            ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gZvfid1RlrtA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020811865623232284"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQDhGwg4lrtC"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yfwIaixHlrtI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LlDT6eDUlrtL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, opt, nb_epochs, metrics, batch_size):\n",
    "    for _ in trange(nb_epochs):\n",
    "        step = len(metrics['train_loss']) + 1\n",
    "        batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "        batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "        batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "    \n",
    "        loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "        opt.zero_grad()\n",
    "        loss_t.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        metrics['train_loss'].append((step, loss_t.item()))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "            \n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                plt.subplot(1, len(metrics), i + 1)\n",
    "                plt.title(name)\n",
    "                plt.plot(*zip(*history))\n",
    "                plt.grid()\n",
    "            plt.show()\n",
    "            print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CklO99wNieLm"
   },
   "outputs": [],
   "source": [
    "metrics = train(model, opt, 25000, metrics, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ahuhKVhlrtP"
   },
   "outputs": [],
   "source": [
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 15, \"We expect a higher BLEU from you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyaHOpealrtS"
   },
   "outputs": [],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv0s8qxOXp5y"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz9aROAIlrtX"
   },
   "source": [
    "### Attention layer (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i_KvKRAM7Yoi"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, name, enc_size, dec_size, hid_size, activ=torch.tanh):\n",
    "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size # num units in encoder state\n",
    "        self.dec_size = dec_size # num units in decoder state\n",
    "        self.hid_size = hid_size # attention layer hidden units\n",
    "        self.activ = activ       # attention layer hidden nonlinearity\n",
    "        \n",
    "        # create trainable paramteres like this:\n",
    "        #self.<PARAMETER_NAME> = nn.Parameter(<INITIAL_VALUES>, requires_grad=True)\n",
    "        self.linear_enc = nn.Linear(enc_size, hid_size)\n",
    "        self.linear_dec = nn.Linear(dec_size, hid_size)\n",
    "        self.linear_out = nn.Linear(hid_size, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute logits\n",
    "        lin_enc = self.linear_enc(enc)\n",
    "        lin_dec = self.linear_dec(dec).unsqueeze(1).repeat([1, enc.size(1), 1])\n",
    "        logits = self.linear_out(self.activ(lin_enc + lin_dec)).squeeze(-1)\n",
    "\n",
    "        # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
    "        # You may need torch.where\n",
    "        neg_inf = torch.full_like(logits, float('-inf'))\n",
    "        logits_masked = torch.where(inp_mask != 0, logits, neg_inf)\n",
    "\n",
    "        # Compute attention probabilities (softmax)\n",
    "        probs = F.softmax(logits_masked, dim=1)\n",
    "\n",
    "        # Compute attention response using enc and probs\n",
    "        attn = torch.sum(enc * probs.unsqueeze(2), dim=1)\n",
    "\n",
    "        return attn, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IalfpdAelrtb"
   },
   "source": [
    "### Seq2seq model with attention (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NCKPB5JmcE6j"
   },
   "outputs": [],
   "source": [
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
    "        self.attention = AttentionLayer('attention', hid_size, hid_size, attn_size)\n",
    "        self.logits = nn.Linear(2*hid_size, len(out_voc))\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode input sequence, create initial decoder states\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        mask = self.inp_voc.compute_mask(inp)\n",
    "        _, first_attn_probas = self.attention(enc_seq, dec_start, mask)\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        first_state = [dec_start, enc_seq, mask, first_attn_probas]\n",
    "        return first_state\n",
    "   \n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_gru0_state = prev_state[0]\n",
    "        enc_seq = prev_state[1]\n",
    "        mask = prev_state[2]\n",
    "        \n",
    "        prev_ix = self.emb_out(prev_tokens)\n",
    "        output = self.dec0(prev_ix, prev_gru0_state)\n",
    "        attn, probas = self.attention(enc_seq, output, mask)\n",
    "        output_attn = torch.cat((output, attn), dim=1)\n",
    "        output_logits = self.logits(output_attn)\n",
    "\n",
    "        new_dec_state = [output, enc_seq, mask, probas]\n",
    "        return new_dec_state, output_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryZCOTEslrtf"
   },
   "source": [
    "### Training attentive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YMHPgZxcFaQ"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = AttentiveModel('en_fr', inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN9gPqDx7Yoj"
   },
   "outputs": [],
   "source": [
    "metrics = train(model, opt, 10000, metrics, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6fKZVnS7Yoj"
   },
   "outputs": [],
   "source": [
    "print(\"BLEU:\", np.mean(metrics['dev_bleu'][-10:], axis=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYoXEGtvLT94"
   },
   "outputs": [],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHT-gNbP7Yok"
   },
   "source": [
    "### Visualizing model attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ILDtVDyO7Yok"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33689/1685393851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "import bokeh.plotting as pl\n",
    "import bokeh.models as bm\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "def draw_attention(inp_line, translation, probs):\n",
    "    \"\"\" An intentionally ambiguous function to visualize attention weights \"\"\"\n",
    "    inp_tokens = inp_voc.tokenize(inp_line)\n",
    "    trans_tokens = out_voc.tokenize(translation)\n",
    "    probs = probs[:len(trans_tokens), :len(inp_tokens)]\n",
    "    \n",
    "    fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),\n",
    "                    x_axis_type=None, y_axis_type=None, tools=[])\n",
    "    fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))\n",
    "\n",
    "    fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')\n",
    "    fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5\n",
    "    fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))\n",
    "    fig.xaxis.major_label_orientation = 45\n",
    "\n",
    "    fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')\n",
    "    fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5\n",
    "    fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))\n",
    "\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ-opYli7Yok"
   },
   "outputs": [],
   "source": [
    "inp = dev_inp[::500]\n",
    "\n",
    "trans, states = model.translate_lines(inp)\n",
    "\n",
    "# select attention probs from model state (you may need to change this for your custom model)\n",
    "# attention_probs below must have shape [batch_size, translation_length, input_length], extracted from states\n",
    "# e.g. if attention probs are at the end of each state, use np.stack([state[-1] for state in states], axis=1)\n",
    "attention_probs = np.stack([state[-1].cpu().detach() for state in states], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD6VZgp47Yok"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    draw_attention(inp[i], trans[i], attention_probs[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbIIngNVlrtt"
   },
   "source": [
    "## Goind deeper (2++ points each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "hy9rW2V24gke"
   },
   "outputs": [],
   "source": [
    "class EnhancedModel(AttentiveModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        super().__init__(name, inp_voc, out_voc, emb_size, hid_size, attn_size) \n",
    "\n",
    "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
    "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
    "        beam_size = 1\n",
    "        if \"beam_size\" in flags:\n",
    "            beam_size = flags[\"beam_size\"]\n",
    "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
    "        state = [initial_state[0]] * beam_size\n",
    "        beams = torch.zeros([batch_size, beam_size, max_len], dtype=torch.int64, device=device)\n",
    "\n",
    "        new_words = torch.full([batch_size, beam_size], self.out_voc.bos_ix, dtype=torch.int64, device=device)\n",
    "        logits = torch.zeros([batch_size, beam_size, len(out_voc)], dtype=torch.float32, device=device)\n",
    "        eos_logits = torch.zeros([batch_size, beam_size], dtype=torch.float32, device=device)\n",
    "        for i in range(max_len):\n",
    "            for j in range(beam_size):\n",
    "                tmp_state = [state[j]] + initial_state[1:]\n",
    "                tmp_state, logits[:,j,:] = self.decode_step(tmp_state, new_words[:,j])\n",
    "                state[j] = tmp_state[0]\n",
    "            max_logits = logits.max(dim=1) # all beams combined\n",
    "            new_words = max_logits.values.topk(beam_size, dim=-1).indices # beam_size new words\n",
    "\n",
    "            # save eos logits\n",
    "            eos_indexes = new_words == out_voc.eos_ix\n",
    "            eos_logits[eos_indexes] = logits[:,:,out_voc.eos_ix][eos_indexes]\n",
    "\n",
    "            # change beam order according to new_words choice\n",
    "            from_beams = torch.gather(max_logits.indices, 1, new_words) # to which sentences these words belong\n",
    "            # ^-- shape: [batch_size, beam_size]\n",
    "            for j in range(batch_size):\n",
    "                for k in range(beam_size):\n",
    "                    state[k][j] = state[from_beams[j,k]][j]\n",
    "            from_beams = from_beams.unsqueeze(2).repeat(1,1,max_len)\n",
    "            beams = torch.gather(beams, 1, from_beams)\n",
    "\n",
    "            # add new words\n",
    "            beams[:, :, i] = new_words\n",
    "        \n",
    "        indexes = eos_logits.argmax(dim=1)\n",
    "        indexes = indexes.unsqueeze(1).unsqueeze(2).repeat(1, beam_size, max_len)\n",
    "        out = torch.gather(beams, 1, indexes)[:,0,:]\n",
    "        \n",
    "        return out, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "A4fQB5BTmQRr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations without training:\n",
      "mouse women pretty pretty pretty pretty helps hamburgers insec@@ pretty pretty por@@ notebooks after@@ leans leans case leans business rou@@ gowns gowns gowns gowns gowns\n",
      "pad ckled ckled ckled ckled butt@@ america ckled america ken@@ wallet ically can seattle pri@@ pri@@ re-@@ re-@@ re-@@ re-@@ re-@@ re-@@ re-@@ re-@@ nati@@\n",
      "able ckled ckled environment environment environment environment environment environment environment environment environment environment environment environment environment environment environment environment environment environment environment environment environment rese@@\n"
     ]
    }
   ],
   "source": [
    "model = EnhancedModel('en_fr', inp_voc, out_voc).to(device)\n",
    "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25, beam_size=10)\n",
    "print(\"Translations without training:\")\n",
    "print('\\n'.join([line for line in dummy_translations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Uoy-onYMKZBi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = EnhancedModel('en_fr', inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Uag0hgD2wrv0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss=0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████████████████▋                                              | 6199/10000 [2:12:16<1:21:06,  1.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33689/2517979286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_33689/1802572045.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, opt, nb_epochs, metrics, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev_bleu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33689/907409040.py\u001b[0m in \u001b[0;36mcompute_bleu\u001b[0;34m(model, inp_lines, out_lines, bpe_sep, **flags)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33689/3922551763.py\u001b[0m in \u001b[0;36mtranslate_lines\u001b[0;34m(self, inp_lines, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mout_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33689/3896235089.py\u001b[0m in \u001b[0;36mdecode_inference\u001b[0;34m(self, initial_state, max_len, **flags)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mtmp_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mtmp_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmax_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# all beams combined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33689/1473790444.py\u001b[0m in \u001b[0;36mdecode_step\u001b[0;34m(self, prev_state, prev_tokens, **flags)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0moutput_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0moutput_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mnew_dec_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = train(model, opt, 10000, metrics, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "fBt1uQNbw1IF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 39.59732177760053\n"
     ]
    }
   ],
   "source": [
    "print(\"BLEU:\", np.mean(metrics['dev_bleu'][-10:], axis=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "wAedkMVmw4oB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un petit garçon tirant une remorque verte vêtu d&apos; un sweat-shirt et de bottes .\n",
      "a little boy pulling a motorcycle wearing a sweatshirt while wearing boots .\n",
      "\n",
      "deux filles portant des bonnets courent dans des am@@ as de neige dehors .\n",
      "the two girls wearing hats are running through a snow covered snow . outside .\n",
      "\n",
      "une jeune personne colorie et dessine sur le trottoir avec une craie rose .\n",
      "a girl is smiling while on the curb with a bright green light .\n",
      "\n",
      "une voiture de course passe à toute vitesse sur le circuit .\n",
      "a race race is running on the racetrack .\n",
      "\n",
      "un homme fait fonctionner une machine tandis qu&apos; il travaille dans une usine .\n",
      "a man is operating a machine as he works into a factory .\n",
      "\n",
      "une femme orientale travaillant sur une chaîne de prod@@ u@@ ction .\n",
      "an old oriental woman working on an electric furniture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500],beam_size=2)[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "edk_oVg0lrtW"
   ],
   "name": "DL4NLP_NMT_2021.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
